{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **REMINDER: DO NOT EDIT IF INSIDE CLONED GIT FOLDER**<br>\n",
    "> \\> Go ahead and copy it and paste it in another folder. Work inside the pasted file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: A Data Scientist's most fundamental tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "Today's exercises will be related to chapters 3, 4, 5 from DSFS. The point of these exercises is to refresh your memory on some mathematics and get you comfortable doing computations in code.\n",
    "\n",
    "The exercises today cover:\n",
    "* Basic visualization\n",
    "* Linear algebra\n",
    "* Statistics\n",
    "\n",
    "**Advice**: Some of you may be new to solving problems using code. You may be wondering *what level of detail* I expect in your solutions, your code comments and explanations. **This is the guideline:** Solve the exercises in a manner that allows you to—later in life—use them as examples. This also means that you should add code comments when the code isn't self-explanatory or if you're afraid it won't make sense when you look at it with fresh eyes. You may also want to comment on your output in plain text to capture the conclusions you arrive at throughout your analysis. But express yourself succinctly. To quote (probably) Einstein: *\"Make everything as simple as possible, but not simpler\"*. Finally, when you optimize for your own future comprehension, other people will be able to understand what you did.\n",
    "\n",
    "[**Feedback**](http://ulfaslak.com/vent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Visualization (DSFS Chapter 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.1**: The figure below meets the minumum style requirements which I expect the figures you make in this class (and life in general) should also meet:\n",
    "* Figure sizing. Try to make the aspect ratio close to 4:3.\n",
    "* Axis labels. Note that you may want to alter the `fontsize` to make them look nice.\n",
    "* Properly sized x and y tick labels.\n",
    "* Title (optional: not always necessary, but oftens helps the reader)\n",
    "* Legend (general rule: only use if you have multiple trends so reader can distinguish).\n",
    ">\n",
    "> Your task in this exercise if to reproduce this figure (perfect match not required).\n",
    ">\n",
    ">*Hint: To get figures to display inside the notebook, use the Jupyter magic `%matplotlib inline`. For pointers on how to make plots like this in Python, Google something like \"scatter plot python\" and see if you can find some examples of how other people do this.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.1.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.2**: The `get_x_y` function below gives you the number of comments versus score for the latest `N` posts on a given `subreddit`.\n",
    "1. Make a scatter plot of `x` vs. `y` for the \"blackmirror\" subreddit (**remember** what you learned in the previous exercise about **styling**). Comment on what you see.\n",
    "2. Maybe you've noticed that it looks pretty bad right? That's because the data does *not scale linearly*! This is a very common thing. To visualize it you should then try to *transform* it somehow. In this case, the data scales *exponentially* in both the x and y direction. Which transformation should we use to make it linear?\n",
    "3. In two seperate figures, floating side by side, scatter plot (left) the set of x and y variables for \"blackmirror\" and (right) x and y for \"news\". Remember to transform the data. My figure looks like [this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.2b.png).\n",
    "4. Comment on any differences you see in the trends. Why might number of comments versus post upvotes look different for a TV-show than for world news?\n",
    ">\n",
    ">*Hint: By \"transformation\" I explicitly mean that you map some function onto every value in a list of values. Example: I can apply a square root transformation like `x = [np.sqrt(v) for v in x]`. A faster way to do that, of course, would be just `x = np.sqrt(x)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T10:04:22.740804Z",
     "start_time": "2019-11-07T10:04:09.189448Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "\n",
    "def get_x_y(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    x, y = [], []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            x.append(d['data']['num_comments'])\n",
    "            y.append(d['data']['score'])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return x, y\n",
    "                          \n",
    "x, y = get_x_y(\"blackmirror\", 500, count=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.1.3**: There is clearly a huge level unevenness in the distribution of how likes and comments given to different posts. Let's visualize this using histograms!\n",
    "1. Log transform `y` (e.g. create a new variable called `y_transformed`) and input it to `plt.hist`. Notice that of there are zeros in `y`, `np.log` will convert them to `-inf`, which `plt.hist` can't handle. You should therefore remove zeros before log transforming. When you have done this, execute `hist_output = plt.hist(x_transformed)`. This should produce a histogram. But what does the variable `hist_output` contain?\n",
    "2. Use `hist_output` to make a similar histogram with the `plt.bar` plotting function. I make you do this to force\n",
    "into your permanent memory what a histogram is: a bar chart showing counts within intervals/bins.\n",
    "3. Plot the distributions of `y_transformed` for \"blackmirror\" and \"news\" as histograms, side by side (you can just use the regular `plt.hist` function here). My figure looks like [this](http://ulfaslak.com/computational_analysis_of_big_data/exer_figures/example_2.2c.png). Comment on the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Linear algebra (DSFS Chapter 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.1**: What does Joel (book) mean when he uses the word *vector*? What does [Grant](https://youtu.be/fNk_zzaMoSs) mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.2**: Using `numpy`, compute:\n",
    "1. `2 * [2, 3]`,\n",
    "2. `[3, 8] + [6, 1]`,\n",
    "3. `[3, 8] * [6, 1]` and\n",
    "4. `[3, 8] · [6, 1]` (dot product)\n",
    "5. `[3, 8, 0] x [6, 1, 0]` (cross product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.3**: Say you have two vectors. What does it mean that the dot product between them is zero or very close to zero? What if it's very large? Intuitively, what does the dot product then measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.4**: In Data Science, we often think of matrices as (usually two-dimensional) containers for data. Let's say we collect $N=500$ data points, that each has $M=10$ features. We can losslessly represent that data using an $N \\times M$ matrix, that is a matrix that has a row for each datapoint and a column for each feature. In fact, let's just go ahead and do that by altering the code of the `get_x_y` function from before a little bit.\n",
    ">\n",
    ">*Note: `numpy` has an object type called `matrix` but we rarely use that. Instead, we represent matrices as a `numpy` object type called `array`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T07:49:55.947679Z",
     "start_time": "2019-01-18T07:49:46.008308Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_matrix(subreddit, N, count=25):\n",
    "    \n",
    "    def _get_data(subreddit, count, after):\n",
    "        url = \"https://www.reddit.com/r/%s/.json?count=%d&after=%s\" % (subreddit, count, after)\n",
    "        data = rq.get(url, headers = {'User-agent': 'sneakybot'}).json()\n",
    "        print(\"Retrieved %d posts from page %s\" % (count, after))\n",
    "        return data\n",
    "    \n",
    "    after = \"\"\n",
    "\n",
    "    X = []\n",
    "    for n in range(N//count):\n",
    "        data = _get_data(subreddit, count, after)\n",
    "        for d in data['data']['children']:\n",
    "            X.append([                                  ### FEATURES ###\n",
    "                int(d['data']['num_comments']),         # Number of comments\n",
    "                int(d['data']['score']),                # Post score (up - down)\n",
    "                int(d['data']['ups']),                  # Post upvotes\n",
    "                int(d['data']['downs']),                # Post downvotes\n",
    "                len(d['data']['selftext']),             # Post text length\n",
    "                len(d['data']['title']),                # Post title length\n",
    "                int(d['data']['is_original_content']),  # Is post original content?\n",
    "                int(d['data']['spoiler']),              # Is post spoiler?\n",
    "                int(d['data']['num_crossposts']),       # Is post crosspost (posted on multiple subreddits)?\n",
    "                int(d['data']['is_video'])              # Is content video?\n",
    "            ])\n",
    "        after = data['data']['after']\n",
    "\n",
    "    return np.array(X)\n",
    "                          \n",
    "X = get_data_matrix(\"news\", 500, count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T07:49:57.572095Z",
     "start_time": "2019-01-18T07:49:57.567762Z"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">So this dataset has $N=178$ rows and $M=13$ columns. Let's start by finding the so-called *covariance matrix* of the features. It is a square, in this case, $13\\times13$ matrix where every value $i,j$ scores the covariance between features $i$ and $j$. [Read more here](https://en.wikipedia.org/wiki/Covariance_matrix).\n",
    "1. Use the `np.cov` method on `X` to get its $13 \\times 13$ covariance matrix.\n",
    "2. Do you notice any characteristics of how values are organized in this matrix? Comment.\n",
    "3. Plot the distribution of covariances, e.g. using a histogram.\n",
    "4. Plot the distribution of correlations in the same way. Comment on the differences between these two plots. Is one easier to interpret than the other?\n",
    ">\n",
    ">*Hint 1: `np.cov` expects that rows are features and columns are observations. That is the transpose of how `X` \n",
    "is represented now.*<br>\n",
    ">*Hint 3: DO NOT just do something like `plt.hist(my_cov_matrix)`. Not only will it reflect that you don't understand\n",
    "what the covariance matrix is, it will also break your kernel.* <br>\n",
    ">*Hint 4: The correlation matrix can be obtained with the `np.corrcoef` function.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.2.5**: There's another use of the covariance matrix, other than just learning how features co-vary. In fact, it turns out that the *eigenvectors* of the covariance matrix are a set of mutually orthogonal vectors, that point in the directions of greatest variance in the data. The eigenvector with the greatest *eigenvalue* points along the direction of greatest variation, and so on. This is pretty neat, because if we know along which axes the data is most stretched, we can figure out how best to project it when visualizing it in 2D as a scatter plot! This whole procedure has a name: **Principal Component Analysis** (PCA) and it was invented by Karl Pearson in 1901. It belongs to a powerful class of linear algebra methods called **Matrix Factorization** methods. Ok, so rather than spending too much time on the math of PCA, let's just use the `sklearn` implementation and fit a PCA on `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-18T08:30:52.583430Z",
     "start_time": "2019-01-18T08:30:52.577358Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. Explain what the matrix you get when you call `pca.components_` means.\n",
    "2. Make a bar plot of `pca.explained_variance_ratio_` and explain what it means (you may want to log-scale the y-axis). What insights about our data can we extract from this?\n",
    "3. Indeed, problem with the data AS-IS, is that the different features have very different variances (some are huge numbers others are small). The way to fix this is by doing something called \"[zscoring](https://en.wikipedia.org/wiki/Standard_score)\", whereby each feature is rescaled to have zero mean and unit standard deviation. In this way, all of the data ends up with comparable variance. Make a new array `X_z` that is the zscored `X`, using the `scipy.stats.zscore` function. Show that each column has zero mean and unit standard deviation.\n",
    "4. Transform `X` using the PCA we fitted above to create a new array `X_pca`. Then fit a new PCA to `X_z` and transform it to create another new array `X_z_pca`. Finally, scatter plot against each other the first two components (i.e. fitst two columns in array) of `X_pca`. Do the same for `X_z_pc`. Comment on the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Statistics (DSFS Chapter 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.1**: Take a vector `a = [1, 3, 2, 5, 3, 1, 5, 1, 9000]`:\n",
    "1. Compute the mean of `a` using `numpy`.\n",
    "2. How is median defined? Compute the median of `a` using `numpy`.\n",
    "3. For `a`, why might it make sense to take the median more seriously than the mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.2**: Using the same vector `a`:\n",
    "1. How is *range* defined? Compute it.\n",
    "2. How is *variance* defined? How do variance and standard deviation relate? Compute them both. Which value is greater?\n",
    "3. What is the interquartile range? Compute it, and explain why it might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.3**: Covariance and correlation are both measures of trend similarity.\n",
    "1. How do they relate?\n",
    "2. Compute the correlation between `a` and `b = [0, 4, 1, 6, 2, 0, 6, 0, 2]`.\n",
    "3. How does that result change if you remove the last data-point from each list? Why? What *term* do we use for that last value for both `a` and `b`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 2.3.4.EXTRA**: People often use the p-value to gauge the *significance* of a given result. If the p-value of a result is low, the result is significant (which is good) and vice versa. Intuitively, the p-value measures the probabilty that a result *could have been obtained at random*, so you can imagine that if you find that the p-value for some result is HIGH (close to one), regardless of how cool it is, people will not care because, well, you just got lucky with that measurement, didn't you? I created two lists for you below, and you are going to find out if they are *significantly* correlated. You will be using the significance threshold 0.05 (which is arbitrary, disputed, yet very standard in the literature).\n",
    ">\n",
    ">***This exercise will not be included in the assignment. But, you can earn up to 10 extra credit points if you solve it and submit it to the teacher. Your solution must reflect that you have understood the idea numerical hypothesis testing.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T09:50:21.670665Z",
     "start_time": "2019-01-24T09:50:21.666647Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can incresae this to make the data more noisy (but let it be 4 for now)\n",
    "noise_level = 4\n",
    "\n",
    "# I'm just seeding the random number generator here, so we can compare results\n",
    "np.random.seed(0)\n",
    "\n",
    "# This is your data\n",
    "x = np.arange(0, 20) + np.random.normal(size=20) * noise_level\n",
    "y = np.arange(0, 20) + np.random.normal(size=20) * noise_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T09:55:18.924607Z",
     "start_time": "2019-01-24T09:55:18.922022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Why not make a scatter plot here, to see what you're working with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1. If you plotted `x` and `y` against each other, you probably noted that it looks like they correlate. Use the method `pearsonr` from `scipy.stats` to **compute the correlation coefficient**. Notice that the method also gives you the p-value of the correlation. For now, we ignore this.\n",
    ">2. So how do we figure out if this correlation strength is significant? \n",
    "The devil's advocate would surely argue, that you could obtain a similar *or stronger* correlation between completely random data.\n",
    "Well, screw you advocate, because we can SHOW that if you randomize THIS data, the correlation strength is almost always lower!\n",
    "To put this in stat-lingo, the devil's advocate believes in the so-called *Null Hypothesis*–that your result is no different from random–and the way we REJECT this acvocate's pessimistic hypothesis, is simply by comparing OUR obtained correlation strength to one obtained from data we know is random, namely the *Null model*.\n",
    "The Null model, in this case just a randomized version of our existing data. \n",
    "Your job, now, is to take `x` and `y`, randomize them (each independently) and compute the correlation coefficient.\n",
    ">3. In 2. you probably found that the correlation coefficient of the Null model was lower than that of your real data.\n",
    "But that was just one example of the Null hypothesis being wrong.\n",
    "The devil's advocate is not convinced and wants more evidence.\n",
    "Compute again the correlation coefficient of the Null model, but do it in a `for` loop 10000 times, and report the fraction of times that this correlation coefficient is greater or equal to the correlation coefficient of your real data.\n",
    "Maybe it will happen in 0% of randomizaion trials, maybe 2%, but hopefully not more than 5%.\n",
    "Yes, you guessed, it, this fraction is indeed the p-value.\n",
    "So is it really significant?\n",
    ">4. Complete the function below that takes as input `noise_level` and computes a p-value. Let `noise_level` vary between 1 and 50 and plot the p-value as a function of `noise_level` so we can see how our result becomes less and less significant as we increase the noise. Two questions: (1) around which value of `noise_level` does the correlation become insignificant, and (2) which p-value does the curve saturate at for large `noise_level`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
